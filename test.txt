This advantage of my framework is that it's fully based on OpenAI assistance API and don't answer your question right now. No, we're not going to support any open source models. Right, so agency's worm does support open source models now. I still recommend you guys go with OpenAI whenever you can, but you know, if you want to use Anthropic, Google Gemini, Mixtrel, or even Lama tree model running locally with Olamma, you now have the flexibility to do so. And I was actually really surprised how simple it is to set up, but the best part is that you can even combine different agents with different LLMs. Like, for example, in this video, we're going to create a manager agent that uses Anthropic and controls two other Google Gemini and Lama tree agents. Let's dive in. Okay, before we dive into the details on how to use my framework with Open Source models, we definitely have to talk about assistance API V2. It released a ton of new features that all significantly improve how your agents perform in production applications. So let me go through all of them one at a time and explain how you can already use them within my framework. Okay, first of all, the Retrieval Tool was replaced with File Search. That's actually huge because before, as I said in my previous video, the Retrieval Tool in OpenAI Assistance API is currently now you get essentially like enterprise level rack out of the box. This has been a game changer for almost all of my clients because now you don't have to set up any data processing yourself. You don't have to connect to third party vector databases. All you have to do is just attach files to your assistant and then OpenAI will do all the processing, all the chunking for you and upload that data into your own vector store. So some of the techniques that they're currently using include like Curie rewriting, breaking down complex curious into multiple searches that can be run in parallel using both keyword and semantic searches and additionally re-ranking the results. So as I said, this is all of the techniques that we use in production level rack applications, like straight out out of the box without you having to worry about any of the details. Okay, next huge update is that now you can finally control the maximum number of tokens that assistant uses in a specific run. So what does this mean? Well, before, you know, you couldn't really control how many tokens were in the prompt when you were using a assistance API. Now, basically what you can do is simply add max prompt tokens parameters inside my framework. So then you can set it to, for example, 2048, although OpenAI does recommend setting it to more than 20,000 if you're using any file searches. Additionally, there is now a new parameter called truncation strategy, which allows you to select how you want those prompt tokens to be truncated. So if you set it to auto, OpenAI says that they're using their own algorithm and from what I've observed, it seems like they're keeping like the first few messages and then they're also keeping the end of the conversation, but they're removing the middle of the discussion. So this allows you apparently to achieve slightly better results because the first few messages typically play a crucial role in how the conversation will unfold. So I do recommend you keep it at auto because OpenAI probably knows what they're doing, but you can also set it to last messages if you want. So if you make truncation strategy an object and then inside you specify type to last messages, you can then select how many messages will be kept in the conversation history. This means that only the five last messages will be used in order to generate a response by the assistant. Additionally, you can of course also select the max completion tokens, which will limit the number of tokens that your assistant can generate at once. As you can see, all of those parameters are actually under the run and point, not under the assistant and point. However, in my frame or you can specify them directly within each assistant. In my opinion, it's more convenient this way because typically those parameters don't alternate between different messages as much. Moreover, you can set all of those parameters even within the agency class. So now as you can see in the latest version of my framework, you can also specify the max prompt tokens within the agency directly and this parameter will act sort of as a default for all your agents. So for example, if I set the max prompt tokens parameter inside the agency class to 2048, all of the agents will use 2048 tokens in the conversation by default. However, if I set this parameter to 4000 within the planner agent, the planner agent will actually use 4000 tokens. The same principle still applies to all the other new parameters like truncation strategy and temperature. By the way, temperature is probably one of the most useful updates from the whole assistance V2 API release because it seems like openly I actually used one for temperature by default in assistance API V1, which is in my opinion a bit too much. You see in production applications, especially in agentic applications, we almost never set temperature to one because in my opinion, creativity is a lot less important than hallucinations. So if you reduce the temperature to something like 0 or 0.3, you're going to reduce the hallucinations. But at the same time, obviously your outputs wouldn't be as diverse. In my framework currently, the default is set to 0.3, however for any coding use cases, I do recommend that you set it to 0. Okay, next we got tool choice, meaning that you can finally select which tool the assistant will use. This is primarily going to be most useful for back-end integrations where you are using the get completion method. So now after you specify the message, you can also specify the tool choice within this method. Then you can select it as auto required or you can set type 2 function and then specify the function choice. So in this example, as you can see, I'm forcing the agent to use the send message tool, which means that this agent will be required to talk to another agent when I say hi. Next, we finally get a way to add assistant messages to threats, which means that now we can finally do few short learning. So let me show you how this works. You can find this under advanced usage agents section in the documentation for agency's form, where I've added a section on how to use few short examples with my framework. So essentially, you just have to create an array of messages that you want your agent to remember. Then you can pass these examples inside the example parameter for your agent. Where this can be mostly useful is if, for example, you want to create emails or any other text in your own style of writing. So then you can say, for example, please create an email for me that answers the following question. And as the assistant responds, you can simply include the email yourself that you have written before. This way, the model will remember your style of writing and will try to replicate it in future messages. However, keep in mind that if the truncation strategy for your agent is said to last messages and the number of last messages exceeds the number of messages in the conversation, it means that your examples might not be included. Next, we have JSON mode, which means that you can force your assistant to always response in JSON object format if you specify type to JSON object. However, keep in mind that you must also include in the instructions that the agent must respond using JSON. Otherwise, this parameter will not work. And last but not least, you can now use fine-tuned models directly within each assistant. You can check any fine-tuned models that you've created by going to the play ground. Here, under the model parameter, you will see all the fine-tuned models. You can simply then copy this model name and paste it inside the model parameter for your agent. Then the agent will use this fine-tuned model. That's it, guys, for all the new assistants V2 updates, keep in mind that all of them are using the exact same format in my framework as in the official OpenAI API. Now, finally, let me show you how to use my framework with open-source models or with any models as a matter of fact. Okay, so the first step is to install my agency's warm lab repository. Where by the time you watch this video, you'll find a new open-source swarm which you can use as a starting point for your project. Just make sure to follow all of the steps from the readme first you need to clone this repo, then install the main requirements from the main requirements.txt file in the root folder and then you're basically ready to go. So, after that, let's navigate into the open-source swarm repository and here you need to install additional requirements file. So, this requirements file actually uses agency's warm 017 version because currently the repository that we're going to be using to emulate assistance API is not very stable with trimming. I'm hoping that it's going to change soon, but for now we're going to have to use one of the previous versions agency's warm which means that also most of the assistants V2 features are currently not available. Once they become available, I'll of course update this repository and send a notification in our Discord. So, simply run peepinstall-r requirements.txt from the open-source swarm directory. After the installation is completed, you will find another litelllmconfig.yaml file. In this file, we will set all the API keys and models that we will use with litelllm. So, litelllm is essentially a library that allows you to create an open-AI proxy server that you can use with any open source or commercial models. So, in this example, I'm going to be using Anthropic, Gemini, Mistrel, provided by TogetherAI, Grog, and of course, Olama, running locally with Lama3. So, you can copy this file and simply set all the API keys for any providers that you want to use. For Olama, you obviously don't have to set any API keys, so you can put one to three, four or anything else. Awesome, so because we want to run the latest Lama3 model locally, we'll also have to install Olama. The installation process is actually really simple. All you have to do is just download the file and then just follow all the installation steps. After the installation process is completed, all you have to do is simply run Olama, run Lama3 command. Then you will see a chat interface like this and you can simply say hi. As you can see, we get this nice response and we can also, for example, ask what LLM are you. Awesome, so as you can see, it indeed says that this is Lama developed by MetaAI, which means that we can proceed to the next step. So the next step is to actually run our Lite LLM proxy server, which you can do with the following command. Lite LLM can thick and then you insert the name of your config YAML file. This will activate our proxy server on localhost port 4000 with all of the models that you specified in your Lama config file. Awesome, so the next step is to pull the open assistant API repository. This is the repository that will essentially mimic the assistant's API backend on our local machine. It supports almost all of the features that open the AI assistant's API supports natively and even provide some additional features. Like, for example, you can already use web browsing. If you provide your Bing subscription key, however, as I said, some features are still a bit unstable. For example, I found a bug that prevents using this backend with custom tools. So I've already fixed this back and created my own fork, which hopefully is going to be merged shortly. For now, however, all you need to do is simply pull my fork instead of the main repo, and then we can proceed with running this backend. You will find all of those detailed instructions inside the open source swarm directory in agency swarm lab. To pull my fork, you can simply use the following git clone command. After it has been copied locally, you will see this folder here on the left. Now the next step is to replace our URLs inside the Docker Compose file. Scroll down a bit until you see the OpenAI base parameter under the LLM config. Here, you need to insert the following URL, HTTPhost.docker.internal port 4000 slash v1. We're going to be using a specialhost.docker.internal URL because it maps the address from within the Docker container to our local host URL on our host machine. For the API key, you can simply set anything like 1, 2, 3, 4 because we're using the LLM. Okay, next let's navigate into this open assistance API repository, and then all you have to do is simply run Docker Compose app-d command. This will build your container and start it as a background process. It will take some time to build the first time you run it, but then the OpenAI assists API will always be running on your local machine in the background, which is extremely convenient. If you don't have Docker installed, the installation is very simple. I've already shown this in one of my previous videos. Simply follow the link from the readme in open source swarm directory and then install it for whichever OS you use. Okay, cool. So now that our assistance API back into Zranian, it's time to set up our agents. Again, navigating to the open source swarm directory, and then I'm going to use the agency swarm create agent template command. I'm going to start with the basic CEO agent. Then I'm going to create a LLMA agent. And finally, I want to create another Google Gemini agent. Cool. So after our agent template folders have been created, you can open them on the left. So let's start with the CEO agent. For the CEO, I want to use the latest cloth model by Anthropic. So all you have to do is simply copy the model name that you previously set in your LLM config. For the LLMA agent, obviously, I'm going to be using all LLMA tree model. And for the Google Gemini agent, I'm going to be using the Gemini Pro 1.5 latest. Cool. So that's literally almost it. That's all it takes to run my framework now with open source models and the final step is to simply set up the agency.py file. So let's add all our agent imports just like this. Then we have to initialize the opening client with a few different parameters. So first we're going to be setting the base URL to the URL of our open assistance server. The API key you can literally set to anything. And for the default headers, we're going to be using assistance V1 API. Then you just have to set open the client that you've initialized before using the set open the client method from agency's form. Now we're ready to initialize our agents and create our agency structure. So let's initialize all the agents as variables. And then simply set the agency with whatever structure you want to use. So here I'm going to have a CO agent that can communicate to both LLMA agent and the Google Gemini agent. Awesome. The final step is to simply run the demo Gradio method. However, here we're not going to be using the default demo Gradio method from the agency class. Instead, we're going to use the demo Gradio method that you will find inside the open source swarm directory here on the left. So this demo Gradio method is essentially just a previous version which does not utilize streaming by default. And instead, simply print out the messages one by one. Again, as I said, this is because the current open assistance API is not as stable with streaming. However, I'm hoping that this is going to be fixed shortly. Awesome. So now we should be ready to run this agency. Again, navigate into the open source swarm folder and then simply run python agency.py. Also, as you can see, now we get the same Gradio interface. Now let's ask the CO agent who is here. As you can see, it says that it's indeed an artificial intelligence developed by Anthropic and the purpose of this agent is to be a helpful digital assistant. It also includes thoughts here at the top which aren't filtered by my Gradio interface but probably should be. That's interesting. Actually, I was quite surprised on how Anthropic added like the special thinking prompts inside the model responses. So as you can see here, Anthropic can actually print like a special tag and then think before answering the question. So this is not filtered by the demo Gradio interface which is why it might look like it's not even speaking to you on the first sentence. And then it provides the response for the question that you asked before. Now let's tell us the same question from the Lama agent. As you can see, it now sends the message to the Lama agent using the send message tool. As you can see, the Lama agent then answers the question and also includes some background capabilities and the purpose. Cool. The CO then relates all of this information to me. But again, as I said, it shows some of the thoughts in the beginning and then it also has like this special function output, sorry, search quality score parameter which is interesting because it seems like Anthropic actually rates its own function calls. So probably by the time we watch this video, I'll actually add some filtering for that or formatting in the Demogradio interface. Another important thing that you do have to keep in mind is that not all of the open source or commercial models currently support function calling. So for example, Lama 3 by Olamma obviously does not support any function calls which means that you can only use this agent as the very end in your agency chart. Because if you added anywhere in front of your agency chart and allow it to communicate with other agents, it's not going to be able to do this because as you might know, in my framework, all the communication is handled through a special send message tool. Okay, finally, let's add this auto as the Google Gemini agent. Also, as you can see, it now sends message to Google Gemini agent with all the arguments correctly defined, which is great. I mean, yeah, it's Anthropic seems to be actually pretty good with function calls, which is why I decided to use it as the CEO agent. Google Gemini then says that it's a large language model created by Google and also provides some of the key capabilities. So yeah, that's basically it, guys. This is how you use my framework with any open source or commercial models. It's the process is actually quite simple, but as I said, please do keep in mind that function calling is not supported by every single model. And in any agentic system, in my opinion, function calling is obviously the most important feature. So to check which models support function calling and which don't, in Lama, I actually created another function calling support.py file where you can insert your model name and then check if the value is true or false. So all you have to do is simply run this file and this will allow you to check whether this is correct or not. I also tested this with GROC and many other providers like together AI and it seems to be working just fine. As I said, the read me with all of the previous setup instructions will be available inside the open source worm directory in my agency's worm lab repository. I know it was one of the most requested features on our discord, so I'm super excited to see what all you guys create. Thank you for watching and don't forget to subscribe.